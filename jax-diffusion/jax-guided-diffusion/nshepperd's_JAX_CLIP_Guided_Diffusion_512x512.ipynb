{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nshepperd's JAX CLIP Guided Diffusion 512x512.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N42DDQQXj0Oz"
      },
      "source": [
        "# Generates images from text prompts with CLIP guided diffusion.\n",
        "\n",
        "Jax port of Katherine Crowson's notebook (https://colab.research.google.com/drive/1V66mUeJbXrTuQITvJunvnWVn96FEbSI3).\n",
        "\n",
        "See also: 256x256 <https://colab.research.google.com/drive/10dvDxcS4e4anlwpJE2yLBjq0O1vKqxdn>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ3rNuAWAewx"
      },
      "source": [
        "# Check the GPU status\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lNIxGBCXpWYY"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson; nshepperd\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_UVMZCIAq_r"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "!pip install dm-haiku cbor2 ftfy einops\n",
        "!git clone https://github.com/kingoflolz/CLIP_JAX\n",
        "!git clone https://github.com/nshepperd/jax-guided-diffusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zAqFEykBHDL"
      },
      "source": [
        "# Download the diffusion model\n",
        "\n",
        "#!curl -OL 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/512x512_diffusion.pt'\n",
        "!test -f 512x512_diffusion_uncond_finetune_008100.pt || curl -OL 'https://the-eye.eu/public/AI/models/512x512_diffusion_unconditional_ImageNet/512x512_diffusion_uncond_finetune_008100.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBRI1uUj5P4e"
      },
      "source": [
        "# Convert the checkpoint to cbor so we don't have to load pytorch...\n",
        "!test -f 512x512_diffusion_uncond_finetune_008100.cbor || python jax-guided-diffusion/convert_checkpoint.py 512x512_diffusion_uncond_finetune_008100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbrcrhpBPC6"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP_JAX')\n",
        "sys.path.append('./jax-guided-diffusion')\n",
        "\n",
        "import math\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import functools\n",
        "from functools import partial\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jaxtorch\n",
        "from jaxtorch import PRNG, Context, ParamState, Module\n",
        "from einops import rearrange\n",
        "\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "import clip_jax\n",
        "\n",
        "from lib.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from lib.util import cutouts_images, pil_to_tensor, pil_from_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFOa9HBw9N5w"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOj78Yvx8jP"
      },
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "class MakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def key(self):\n",
        "        return (self.cut_size,self.cutn,self.cut_pow)\n",
        "    def __hash__(self):\n",
        "        return hash(self.key())\n",
        "    def __eq__(self, other):\n",
        "        if isinstance(other, MakeCutouts):\n",
        "            return type(self) is type(other) and self.key() == other.key()\n",
        "        return NotImplemented\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [b, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[self.cutn])**self.cut_pow\n",
        "        sizes = (min_size + cut_us * (max_size - min_size + 1)).astype(jnp.int32).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.randint(rng.split(), [self.cutn], 0, w - sizes + 1)\n",
        "        offsets_y = jax.random.randint(rng.split(), [self.cutn], 0, h - sizes + 1)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "        cutouts = cutouts.rearrange('b n c h w -> (n b) c h w')\n",
        "        return cutouts\n",
        "\n",
        "class StaticCutouts(MakeCutouts):\n",
        "    def __init__(self, cut_size, cutn, size):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.size = size\n",
        "\n",
        "    def key(self):\n",
        "        return (self.cut_size,self.cutn,self.size)\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [b, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "        sizes = jnp.array([self.size]*self.cutn).astype(jnp.int32)\n",
        "        offsets_x = jax.random.randint(rng.split(), [self.cutn], 0, w - sizes + 1)\n",
        "        offsets_y = jax.random.randint(rng.split(), [self.cutn], 0, h - sizes + 1)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "        cutouts = cutouts.rearrange('b n c h w -> (n b) c h w')\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "def Normalize(mean, std):\n",
        "    mean = jnp.array(mean).reshape(3,1,1)\n",
        "    std = jnp.array(std).reshape(3,1,1)\n",
        "    def forward(image):\n",
        "        return (image - mean) / std\n",
        "    return forward\n",
        "\n",
        "def norm1(x):\n",
        "    \"\"\"Normalize to the unit sphere.\"\"\"\n",
        "    return x / x.square().sum(axis=-1, keepdims=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = norm1(x)\n",
        "    y = norm1(y)\n",
        "    return (x - y).square().sum(axis=-1).sqrt().div(2).arcsin().square().mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    # input = jnp.pad(input, ((0,0), (0,0), (0,1), (0,1)), mode='edge')\n",
        "    # x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    # y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    # return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "    x_diff = input[..., :, 1:] - input[..., :, :-1]\n",
        "    y_diff = input[..., 1:, :] - input[..., :-1, :]\n",
        "    return x_diff.square().mean([1,2,3]) + y_diff.square().mean([1,2,3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpbody2NCR7w"
      },
      "source": [
        "# Model settings\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '1000',\n",
        "    'image_size': 512,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_fp16': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnQjGugaDZPJ"
      },
      "source": [
        "# Load models\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model_params = ParamState(model.labeled_parameters_())\n",
        "model_params.initialize(jax.random.PRNGKey(0))\n",
        "\n",
        "print('Loading state dict...')\n",
        "with open('512x512_diffusion_uncond_finetune_008100.cbor', 'rb') as fp:\n",
        "    jax_state_dict = jaxtorch.cbor.load(fp)\n",
        "\n",
        "model.load_state_dict(model_params, jax_state_dict)\n",
        "\n",
        "# model_params_f16 = ParamState(model.labeled_parameters_())\n",
        "# for p in model.parameters():\n",
        "#   if model_params[p].dtype == jnp.float32:\n",
        "#     model_params_f16[p] = model_params[p].astype(jnp.bfloat16)\n",
        "#   else:\n",
        "#     model_params_f16[p] = model_params[p]\n",
        "\n",
        "print('Loading CLIP model...')\n",
        "image_fn, text_fn, clip_params, _ = clip_jax.load('ViT-B/32')\n",
        "clip_size = 224\n",
        "normalize = Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                      std=[0.26862954, 0.26130258, 0.27577711])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP6E9kI8GI4Z"
      },
      "source": [
        "\n",
        "def exec_model(model_params, x, timesteps, y=None):\n",
        "    cx = Context(model_params, jax.random.PRNGKey(0))\n",
        "    cx.mode = 'eval'\n",
        "    return model(cx, x, timesteps, y)\n",
        "exec_model = jax.jit(exec_model)\n",
        "exec_model_jit = functools.partial(exec_model, model_params)\n",
        "\n",
        "# def cond_loss(x, t, y, text_embed, style_embed, cur_t, key, model_params, clip_params, clip_guidance_scale, style_guidance_scale, tv_scale, make_cutouts, make_cutouts_style):\n",
        "#     rng = PRNG(key)\n",
        "#     n = x.shape[0]\n",
        "#     my_t = jnp.ones([n], dtype=jnp.int32) * cur_t\n",
        "#     out = diffusion.p_mean_variance(functools.partial(exec_model,model_params),\n",
        "#                                     x, my_t,\n",
        "#                                     clip_denoised=False,\n",
        "#                                     model_kwargs={'y': y})\n",
        "#     fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "#     x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "#     clip_in = normalize(make_cutouts(x_in.add(1).div(2), rng.split()))\n",
        "#     image_embeds = emb_image(clip_in, clip_params).reshape([cutn, n, 512])\n",
        "#     # Method 1. Average the clip embeds, then compute great circle distance.\n",
        "#     # losses = spherical_dist_loss(image_embeds.mean(0), text_embed)\n",
        "#     # Method 2. Compute great circle losses for clip embeds, then average.\n",
        "#     losses = spherical_dist_loss(image_embeds, text_embed).mean(0)\n",
        "#     if style_embed is not None:\n",
        "#       clip_in = normalize(make_cutouts_style(x_in.add(1).div(2), rng.split()))\n",
        "#       image_embeds = emb_image(clip_in, clip_params).reshape([cutn, n, 512])\n",
        "#       style_losses = spherical_dist_loss(image_embeds, style_embed).mean(0)\n",
        "#     else:\n",
        "#       style_losses = jnp.zeros([n])\n",
        "#     tv_losses = tv_loss(x_in)\n",
        "#     loss = losses.sum() * clip_guidance_scale + style_losses.sum() * style_guidance_scale + tv_losses.sum() * tv_scale\n",
        "#     return -loss\n",
        "# base_cond_fn = jax.jit(jax.grad(cond_loss), static_argnames=['make_cutouts', 'make_cutouts_style'])\n",
        "\n",
        "\n",
        "\n",
        "def base_cond_fn(x, t, y, text_embed, style_embed, cur_t, key, model_params, clip_params, clip_guidance_scale, style_guidance_scale, tv_scale, make_cutouts, make_cutouts_style):\n",
        "    rng = PRNG(key)\n",
        "    n = x.shape[0]\n",
        "\n",
        "    def denoise(x):\n",
        "      my_t = jnp.ones([n], dtype=jnp.int32) * cur_t\n",
        "      out = diffusion.p_mean_variance(functools.partial(exec_model,model_params),\n",
        "                                      x, my_t,\n",
        "                                      clip_denoised=False,\n",
        "                                      model_kwargs={'y': y})\n",
        "      fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "      x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "      return x_in\n",
        "    (x_in, backward) = jax.vjp(denoise, x)\n",
        "\n",
        "    def main_clip_loss(x_in, key):\n",
        "      clip_in = normalize(make_cutouts(x_in.add(1).div(2), key))\n",
        "      image_embeds = emb_image(clip_in, clip_params).reshape([make_cutouts.cutn, n, 512])\n",
        "      # Method 1. Average the clip embeds, then compute great circle distance.\n",
        "      # losses = spherical_dist_loss(image_embeds.mean(0), text_embed)\n",
        "      # Method 2. Compute great circle losses for clip embeds, then average.\n",
        "      losses = spherical_dist_loss(image_embeds, text_embed).mean(0)\n",
        "      return losses.sum() * clip_guidance_scale\n",
        "    \n",
        "    # Scan method, should reduce jit times...\n",
        "    num_cuts = 4\n",
        "    keys = jnp.stack([rng.split() for _ in range(num_cuts)])\n",
        "    main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                  jnp.zeros_like(x_in),\n",
        "                                  keys)[0] / num_cuts\n",
        "\n",
        "    if style_embed is not None:\n",
        "      def style_loss(x_in, key):\n",
        "        clip_in = normalize(make_cutouts_style(x_in.add(1).div(2), key))\n",
        "        image_embeds = emb_image(clip_in, clip_params).reshape([make_cutouts_style.cutn, n, 512])\n",
        "        style_losses = spherical_dist_loss(image_embeds, style_embed).mean(0)\n",
        "        return style_losses.sum() * style_guidance_scale\n",
        "      main_clip_grad += jax.grad(style_loss)(x_in, rng.split())\n",
        "\n",
        "    def sum_tv_loss(x_in):\n",
        "      return tv_loss(x_in).sum() * tv_scale\n",
        "    main_clip_grad += jax.grad(sum_tv_loss)(x_in)\n",
        "\n",
        "    return -backward(main_clip_grad)[0]\n",
        "base_cond_fn = jax.jit(base_cond_fn, static_argnames=['make_cutouts', 'make_cutouts_style'])\n",
        "\n",
        "def txt(prompt):\n",
        "  \"\"\"Returns normalized embedding.\"\"\"\n",
        "  text = clip_jax.tokenize([prompt])\n",
        "  text_embed = text_fn(clip_params, text)\n",
        "  return norm1(text_embed.reshape(512))\n",
        "\n",
        "@jax.jit\n",
        "def emb_image(image, clip_params=None):\n",
        "    return norm1(image_fn(clip_params, image))\n",
        "\n",
        "def cborfile(path):\n",
        "    with fetch(path) as fp:\n",
        "      return jaxtorch.cbor.load(fp)\n",
        "\n",
        "def pimage(path):\n",
        "    \"Normalized clip image embedding.\"\n",
        "    fp = fetch(path)\n",
        "    img = Image.open(fp)\n",
        "    img = pil_to_tensor(img)\n",
        "    [c, h, w] = img.shape\n",
        "    img = img.unsqueeze(0)\n",
        "    cut_fn = MakeCutouts(clip_size, cutn=64, cut_pow=0.5)\n",
        "    batch = normalize(cut_fn(img, key=jax.random.PRNGKey(0)))\n",
        "    embed = emb_image(batch, clip_params).mean(0)\n",
        "    return norm1(embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zY-8I90LkC6"
      },
      "source": [
        "## Settings for this run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PwzFZbLfcy"
      },
      "source": [
        "title = 'a beautiful fantasy land forest of glass, trending on ArtStation'\n",
        "prompt = txt(title)\n",
        "style_embed = norm1(jnp.array(cborfile('jax-guided-diffusion/data/openimages_512x_png_embed224.cbor'))) - norm1(jnp.array(cborfile('jax-guided-diffusion/data/imagenet_512x_jpg_embed224.cbor')))\n",
        "batch_size = 1\n",
        "clip_guidance_scale = 2000\n",
        "style_guidance_scale = 300\n",
        "tv_scale = 150\n",
        "cutn = 32 # effective cutn is 4x this because we do 4 iterations in base_cond_fn\n",
        "cut_pow = 0.5\n",
        "style_cutn = 32\n",
        "n_batches = 4\n",
        "init_image = None\n",
        "skip_timesteps = 0\n",
        "seed = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9hTc8YLoLx"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5gODNAMEUCR"
      },
      "source": [
        "def run():\n",
        "    rng = PRNG(jax.random.PRNGKey(seed))\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((model_config['image_size'], model_config['image_size']), Image.LANCZOS)\n",
        "        init = pil_to_tensor(init).rearrange('c h w -> 1 c h w').mul(2).sub(1)\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn, cut_pow=cut_pow)\n",
        "    make_cutouts_style = StaticCutouts(clip_size, style_cutn, size=224)\n",
        "\n",
        "    def cond_fn(x, t, y=None):\n",
        "        # Triggers recompilation if cutout parameters have changed (cutn or cut_pow).\n",
        "        return base_cond_fn(x, jnp.array(t), y,\n",
        "                            text_embed=text_embed,\n",
        "                            style_embed=style_embed,\n",
        "                            cur_t=jnp.array(cur_t),\n",
        "                            key=rng.split(),\n",
        "                            model_params=model_params,\n",
        "                            clip_params=clip_params,\n",
        "                            clip_guidance_scale = clip_guidance_scale,\n",
        "                            style_guidance_scale = style_guidance_scale,\n",
        "                            tv_scale = tv_scale,\n",
        "                            make_cutouts=make_cutouts,\n",
        "                            make_cutouts_style=make_cutouts_style)\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        if type(prompt) is list:\n",
        "          text_embed = prompt[i % len(prompt)]\n",
        "          this_title = title[i % len(prompt)]\n",
        "        else:\n",
        "          text_embed = prompt\n",
        "          this_title = title\n",
        "\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = diffusion.p_sample_loop_progressive(\n",
        "            exec_model_jit,\n",
        "            (batch_size, 3, model_config['image_size'], model_config['image_size']),\n",
        "            rng=rng,\n",
        "            clip_denoised=False,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=tqdm,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init\n",
        "        )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            cur_t -= 1\n",
        "            if j % 100 == 0 or cur_t == -1:\n",
        "                print()\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
        "                    # print(k, type(image).mro())\n",
        "                    # For some reason this comes out as a numpy array. Huh?\n",
        "                    image = pil_from_tensor(jnp.array(image).add(1).div(2))\n",
        "                    image.save(filename)\n",
        "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                    display.display(display.Image(filename))\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            filename = f'progress_{i * batch_size + k:05}.png'\n",
        "            timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "            os.makedirs('samples', exist_ok=True)\n",
        "            dname = f'samples/{timestring}_{k}_{this_title}.png'\n",
        "            with open(filename, 'rb') as fp:\n",
        "              data = fp.read()\n",
        "            with open(dname, 'wb') as fp:\n",
        "              fp.write(data)\n",
        "            if os.path.exists('/content/drive'):\n",
        "              os.makedirs('/content/drive/MyDrive/samples', exist_ok=True)\n",
        "              with open(f'/content/drive/MyDrive/{dname}', 'wb') as fp:\n",
        "                fp.write(data)\n",
        "            files.download(dname)\n",
        "\n",
        "run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}